{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentter\n",
    "##### The objective of this project is to predict the sentiment of the given statement. A voting classifier is designed with the help of three classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['sentiment', 'id', 'date', 'query_string', 'user', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding = 'ISO-8859-1', header = None, names = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Here we see that, there are 80,00,000 positive and 80,00,000 negative pre-labeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id', 'date', 'query_string', 'user'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finding the length of every tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map({0:0, 4:1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Pre-processing of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    \n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "        \n",
    "    except:\n",
    "        clean = stripped\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b' i just received my G8 viola exam.. and its... well... .. disappointing.. :\\\\..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\\\../  \\\\../'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print (\"Cleaning and parsing the tweets...\\n\")\n",
    "clean_tweet_texts = []\n",
    "for i in range(1600000):\n",
    "    \n",
    "    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awww that s a bummer you shoulda got david car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he can t update his facebook by ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no it s not behaving at all i m mad why am i h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  awww that s a bummer you shoulda got david car...       0\n",
       "1  is upset that he can t update his facebook by ...       0\n",
       "2  i dived many times for the ball managed to sav...       0\n",
       "3     my whole body feels itchy and like its on fire       0\n",
       "4  no it s not behaving at all i m mad why am i h...       0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.DataFrame(clean_tweet_texts, columns=['text'])\n",
    "clean_df['target'] = df.sentiment\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Saving the cleaned tweets to a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('clean_tweet.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clean_tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vect = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Splitting the data into training, testing and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.04, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1532882, 273694)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31935, 273694)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31936, 273694)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=42, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y_array_lr = y_test.as_matrix(columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25641"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_lr = 0\n",
    "for i in range(y_test.count()):\n",
    "    if(y_array_lr[i] == y_pred_lr[i]):\n",
    "        same_lr = same_lr+1\n",
    "same_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8029121653358384"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_lr = (same_lr) / float(y_test.count())\n",
    "accuracy_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy of Logistic Regression model is 80.29%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.79      0.80     15868\n",
      "          1       0.80      0.81      0.81     16067\n",
      "\n",
      "avg / total       0.80      0.80      0.80     31935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 470 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mnb = mnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y_array_mnb = y_test.as_matrix(columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24750"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_mnb = 0\n",
    "for i in range(y_test.count()):\n",
    "    if(y_array_mnb[i] == y_pred_mnb[i]):\n",
    "        same_mnb = same_mnb+1\n",
    "same_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7750117426021607"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_mnb = (same_mnb) / float(y_test.count())\n",
    "accuracy_mnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy of Logistic Regression model is 77.50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.79      0.78     15868\n",
      "          1       0.79      0.76      0.77     16067\n",
      "\n",
      "avg / total       0.78      0.78      0.77     31935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating a Stochastic Gradient Descent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = linear_model.SGDClassifier(verbose=2, max_iter=200, loss='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 28.41, NNZs: 267036, Bias: 0.289673, T: 1532882, Avg. loss: 0.515293\n",
      "Total training time: 0.57 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 28.41, NNZs: 267036, Bias: 0.291307, T: 3065764, Avg. loss: 0.514084\n",
      "Total training time: 1.16 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 28.40, NNZs: 267036, Bias: 0.294371, T: 4598646, Avg. loss: 0.513916\n",
      "Total training time: 1.77 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.291386, T: 6131528, Avg. loss: 0.513876\n",
      "Total training time: 2.36 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.293674, T: 7664410, Avg. loss: 0.513868\n",
      "Total training time: 2.94 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.291144, T: 9197292, Avg. loss: 0.513846\n",
      "Total training time: 3.52 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.291457, T: 10730174, Avg. loss: 0.513835\n",
      "Total training time: 4.15 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292255, T: 12263056, Avg. loss: 0.513826\n",
      "Total training time: 4.77 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.293815, T: 13795938, Avg. loss: 0.513815\n",
      "Total training time: 5.38 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.291636, T: 15328820, Avg. loss: 0.513813\n",
      "Total training time: 5.95 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292189, T: 16861702, Avg. loss: 0.513808\n",
      "Total training time: 6.52 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292286, T: 18394584, Avg. loss: 0.513804\n",
      "Total training time: 7.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292004, T: 19927466, Avg. loss: 0.513801\n",
      "Total training time: 7.76 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292167, T: 21460348, Avg. loss: 0.513795\n",
      "Total training time: 8.34 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292405, T: 22993230, Avg. loss: 0.513793\n",
      "Total training time: 8.92 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292563, T: 24526112, Avg. loss: 0.513800\n",
      "Total training time: 9.49 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292312, T: 26058994, Avg. loss: 0.513790\n",
      "Total training time: 10.06 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292292, T: 27591876, Avg. loss: 0.513789\n",
      "Total training time: 10.61 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292224, T: 29124758, Avg. loss: 0.513784\n",
      "Total training time: 11.23 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292501, T: 30657640, Avg. loss: 0.513786\n",
      "Total training time: 11.87 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292729, T: 32190522, Avg. loss: 0.513785\n",
      "Total training time: 12.45 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.293049, T: 33723404, Avg. loss: 0.513785\n",
      "Total training time: 13.09 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292777, T: 35256286, Avg. loss: 0.513785\n",
      "Total training time: 13.72 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292945, T: 36789168, Avg. loss: 0.513787\n",
      "Total training time: 14.35 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292551, T: 38322050, Avg. loss: 0.513784\n",
      "Total training time: 14.92 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292630, T: 39854932, Avg. loss: 0.513780\n",
      "Total training time: 15.48 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292234, T: 41387814, Avg. loss: 0.513780\n",
      "Total training time: 16.02 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292057, T: 42920696, Avg. loss: 0.513782\n",
      "Total training time: 16.60 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292486, T: 44453578, Avg. loss: 0.513777\n",
      "Total training time: 17.17 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292239, T: 45986460, Avg. loss: 0.513779\n",
      "Total training time: 17.74 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292593, T: 47519342, Avg. loss: 0.513775\n",
      "Total training time: 18.30 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292223, T: 49052224, Avg. loss: 0.513779\n",
      "Total training time: 18.86 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292069, T: 50585106, Avg. loss: 0.513780\n",
      "Total training time: 19.42 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292606, T: 52117988, Avg. loss: 0.513775\n",
      "Total training time: 19.99 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292698, T: 53650870, Avg. loss: 0.513773\n",
      "Total training time: 20.64 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292480, T: 55183752, Avg. loss: 0.513779\n",
      "Total training time: 21.22 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292643, T: 56716634, Avg. loss: 0.513774\n",
      "Total training time: 21.79 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292534, T: 58249516, Avg. loss: 0.513775\n",
      "Total training time: 22.39 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292353, T: 59782398, Avg. loss: 0.513775\n",
      "Total training time: 23.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292663, T: 61315280, Avg. loss: 0.513772\n",
      "Total training time: 23.66 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292412, T: 62848162, Avg. loss: 0.513775\n",
      "Total training time: 24.24 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292291, T: 64381044, Avg. loss: 0.513774\n",
      "Total training time: 24.83 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292355, T: 65913926, Avg. loss: 0.513775\n",
      "Total training time: 25.38 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292505, T: 67446808, Avg. loss: 0.513773\n",
      "Total training time: 26.05 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292499, T: 68979690, Avg. loss: 0.513773\n",
      "Total training time: 26.70 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292356, T: 70512572, Avg. loss: 0.513773\n",
      "Total training time: 27.32 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292249, T: 72045454, Avg. loss: 0.513771\n",
      "Total training time: 27.95 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292416, T: 73578336, Avg. loss: 0.513769\n",
      "Total training time: 28.55 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292282, T: 75111218, Avg. loss: 0.513771\n",
      "Total training time: 29.29 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292300, T: 76644100, Avg. loss: 0.513771\n",
      "Total training time: 29.97 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292497, T: 78176982, Avg. loss: 0.513770\n",
      "Total training time: 30.64 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292222, T: 79709864, Avg. loss: 0.513773\n",
      "Total training time: 31.31 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292124, T: 81242746, Avg. loss: 0.513769\n",
      "Total training time: 32.01 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292461, T: 82775628, Avg. loss: 0.513769\n",
      "Total training time: 32.68 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292360, T: 84308510, Avg. loss: 0.513772\n",
      "Total training time: 33.33 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292567, T: 85841392, Avg. loss: 0.513771\n",
      "Total training time: 34.11 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292392, T: 87374274, Avg. loss: 0.513770\n",
      "Total training time: 34.91 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292418, T: 88907156, Avg. loss: 0.513770\n",
      "Total training time: 35.58 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292491, T: 90440038, Avg. loss: 0.513770\n",
      "Total training time: 36.30 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292490, T: 91972920, Avg. loss: 0.513769\n",
      "Total training time: 37.02 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292405, T: 93505802, Avg. loss: 0.513769\n",
      "Total training time: 37.67 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292423, T: 95038684, Avg. loss: 0.513769\n",
      "Total training time: 38.32 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292500, T: 96571566, Avg. loss: 0.513768\n",
      "Total training time: 38.98 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292373, T: 98104448, Avg. loss: 0.513770\n",
      "Total training time: 39.56 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292533, T: 99637330, Avg. loss: 0.513768\n",
      "Total training time: 40.23 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292452, T: 101170212, Avg. loss: 0.513771\n",
      "Total training time: 40.85 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292394, T: 102703094, Avg. loss: 0.513769\n",
      "Total training time: 41.53 seconds.\n",
      "-- Epoch 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 28.38, NNZs: 267036, Bias: 0.292426, T: 104235976, Avg. loss: 0.513769\n",
      "Total training time: 42.20 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292432, T: 105768858, Avg. loss: 0.513770\n",
      "Total training time: 42.82 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292363, T: 107301740, Avg. loss: 0.513770\n",
      "Total training time: 43.46 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292378, T: 108834622, Avg. loss: 0.513771\n",
      "Total training time: 44.06 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292407, T: 110367504, Avg. loss: 0.513769\n",
      "Total training time: 44.69 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292402, T: 111900386, Avg. loss: 0.513769\n",
      "Total training time: 45.33 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292435, T: 113433268, Avg. loss: 0.513768\n",
      "Total training time: 45.93 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292396, T: 114966150, Avg. loss: 0.513769\n",
      "Total training time: 46.54 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292323, T: 116499032, Avg. loss: 0.513768\n",
      "Total training time: 47.13 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292376, T: 118031914, Avg. loss: 0.513769\n",
      "Total training time: 47.70 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292324, T: 119564796, Avg. loss: 0.513768\n",
      "Total training time: 48.32 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292403, T: 121097678, Avg. loss: 0.513767\n",
      "Total training time: 48.87 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292343, T: 122630560, Avg. loss: 0.513768\n",
      "Total training time: 49.44 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292402, T: 124163442, Avg. loss: 0.513769\n",
      "Total training time: 50.01 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292488, T: 125696324, Avg. loss: 0.513769\n",
      "Total training time: 50.64 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292477, T: 127229206, Avg. loss: 0.513768\n",
      "Total training time: 51.22 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292483, T: 128762088, Avg. loss: 0.513768\n",
      "Total training time: 51.86 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292492, T: 130294970, Avg. loss: 0.513768\n",
      "Total training time: 52.47 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292474, T: 131827852, Avg. loss: 0.513768\n",
      "Total training time: 53.04 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292436, T: 133360734, Avg. loss: 0.513768\n",
      "Total training time: 53.70 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292357, T: 134893616, Avg. loss: 0.513768\n",
      "Total training time: 54.31 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292374, T: 136426498, Avg. loss: 0.513768\n",
      "Total training time: 54.90 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292359, T: 137959380, Avg. loss: 0.513767\n",
      "Total training time: 55.58 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292403, T: 139492262, Avg. loss: 0.513766\n",
      "Total training time: 56.22 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292385, T: 141025144, Avg. loss: 0.513766\n",
      "Total training time: 56.83 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292468, T: 142558026, Avg. loss: 0.513766\n",
      "Total training time: 57.44 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292458, T: 144090908, Avg. loss: 0.513766\n",
      "Total training time: 58.04 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292425, T: 145623790, Avg. loss: 0.513768\n",
      "Total training time: 58.64 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292473, T: 147156672, Avg. loss: 0.513767\n",
      "Total training time: 59.29 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292486, T: 148689554, Avg. loss: 0.513767\n",
      "Total training time: 59.93 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292473, T: 150222436, Avg. loss: 0.513768\n",
      "Total training time: 60.51 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292445, T: 151755318, Avg. loss: 0.513767\n",
      "Total training time: 61.11 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292414, T: 153288200, Avg. loss: 0.513767\n",
      "Total training time: 61.71 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292351, T: 154821082, Avg. loss: 0.513767\n",
      "Total training time: 62.29 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292439, T: 156353964, Avg. loss: 0.513766\n",
      "Total training time: 62.86 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292427, T: 157886846, Avg. loss: 0.513767\n",
      "Total training time: 63.48 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292486, T: 159419728, Avg. loss: 0.513766\n",
      "Total training time: 64.05 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292384, T: 160952610, Avg. loss: 0.513767\n",
      "Total training time: 64.62 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292449, T: 162485492, Avg. loss: 0.513766\n",
      "Total training time: 65.18 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292435, T: 164018374, Avg. loss: 0.513766\n",
      "Total training time: 65.74 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292426, T: 165551256, Avg. loss: 0.513767\n",
      "Total training time: 66.29 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292437, T: 167084138, Avg. loss: 0.513766\n",
      "Total training time: 66.84 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292387, T: 168617020, Avg. loss: 0.513766\n",
      "Total training time: 67.41 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292365, T: 170149902, Avg. loss: 0.513765\n",
      "Total training time: 67.96 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292402, T: 171682784, Avg. loss: 0.513765\n",
      "Total training time: 68.51 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292366, T: 173215666, Avg. loss: 0.513767\n",
      "Total training time: 69.06 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292391, T: 174748548, Avg. loss: 0.513767\n",
      "Total training time: 69.65 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292390, T: 176281430, Avg. loss: 0.513767\n",
      "Total training time: 70.28 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292389, T: 177814312, Avg. loss: 0.513767\n",
      "Total training time: 70.84 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292414, T: 179347194, Avg. loss: 0.513766\n",
      "Total training time: 71.38 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292443, T: 180880076, Avg. loss: 0.513766\n",
      "Total training time: 71.94 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292427, T: 182412958, Avg. loss: 0.513766\n",
      "Total training time: 72.49 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292442, T: 183945840, Avg. loss: 0.513766\n",
      "Total training time: 73.06 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292460, T: 185478722, Avg. loss: 0.513765\n",
      "Total training time: 73.63 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292450, T: 187011604, Avg. loss: 0.513767\n",
      "Total training time: 74.34 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292408, T: 188544486, Avg. loss: 0.513766\n",
      "Total training time: 75.02 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292351, T: 190077368, Avg. loss: 0.513765\n",
      "Total training time: 75.71 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292385, T: 191610250, Avg. loss: 0.513766\n",
      "Total training time: 76.36 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292385, T: 193143132, Avg. loss: 0.513766\n",
      "Total training time: 76.97 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292398, T: 194676014, Avg. loss: 0.513765\n",
      "Total training time: 77.58 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292370, T: 196208896, Avg. loss: 0.513766\n",
      "Total training time: 78.23 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292356, T: 197741778, Avg. loss: 0.513766\n",
      "Total training time: 78.87 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292351, T: 199274660, Avg. loss: 0.513765\n",
      "Total training time: 79.49 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292354, T: 200807542, Avg. loss: 0.513765\n",
      "Total training time: 80.12 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292386, T: 202340424, Avg. loss: 0.513765\n",
      "Total training time: 80.74 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292383, T: 203873306, Avg. loss: 0.513766\n",
      "Total training time: 81.31 seconds.\n",
      "-- Epoch 134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 28.38, NNZs: 267036, Bias: 0.292405, T: 205406188, Avg. loss: 0.513765\n",
      "Total training time: 81.86 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292458, T: 206939070, Avg. loss: 0.513764\n",
      "Total training time: 82.43 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292433, T: 208471952, Avg. loss: 0.513765\n",
      "Total training time: 82.98 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292430, T: 210004834, Avg. loss: 0.513765\n",
      "Total training time: 83.53 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292394, T: 211537716, Avg. loss: 0.513766\n",
      "Total training time: 84.17 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292405, T: 213070598, Avg. loss: 0.513766\n",
      "Total training time: 84.78 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292376, T: 214603480, Avg. loss: 0.513766\n",
      "Total training time: 85.45 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292408, T: 216136362, Avg. loss: 0.513765\n",
      "Total training time: 86.03 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292390, T: 217669244, Avg. loss: 0.513766\n",
      "Total training time: 86.62 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292387, T: 219202126, Avg. loss: 0.513765\n",
      "Total training time: 87.19 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292389, T: 220735008, Avg. loss: 0.513765\n",
      "Total training time: 87.79 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292397, T: 222267890, Avg. loss: 0.513766\n",
      "Total training time: 88.39 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292398, T: 223800772, Avg. loss: 0.513766\n",
      "Total training time: 88.99 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292374, T: 225333654, Avg. loss: 0.513766\n",
      "Total training time: 89.61 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292390, T: 226866536, Avg. loss: 0.513765\n",
      "Total training time: 90.21 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292370, T: 228399418, Avg. loss: 0.513765\n",
      "Total training time: 90.87 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292414, T: 229932300, Avg. loss: 0.513765\n",
      "Total training time: 91.48 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292431, T: 231465182, Avg. loss: 0.513765\n",
      "Total training time: 92.10 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292460, T: 232998064, Avg. loss: 0.513765\n",
      "Total training time: 92.75 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292422, T: 234530946, Avg. loss: 0.513765\n",
      "Total training time: 93.39 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292429, T: 236063828, Avg. loss: 0.513764\n",
      "Total training time: 93.95 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292440, T: 237596710, Avg. loss: 0.513766\n",
      "Total training time: 94.54 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292432, T: 239129592, Avg. loss: 0.513764\n",
      "Total training time: 95.18 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292431, T: 240662474, Avg. loss: 0.513765\n",
      "Total training time: 95.74 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292394, T: 242195356, Avg. loss: 0.513764\n",
      "Total training time: 96.36 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292363, T: 243728238, Avg. loss: 0.513765\n",
      "Total training time: 96.98 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292365, T: 245261120, Avg. loss: 0.513765\n",
      "Total training time: 97.61 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292369, T: 246794002, Avg. loss: 0.513765\n",
      "Total training time: 98.20 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292355, T: 248326884, Avg. loss: 0.513765\n",
      "Total training time: 98.85 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292366, T: 249859766, Avg. loss: 0.513765\n",
      "Total training time: 99.45 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292389, T: 251392648, Avg. loss: 0.513764\n",
      "Total training time: 100.07 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292396, T: 252925530, Avg. loss: 0.513765\n",
      "Total training time: 100.65 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292380, T: 254458412, Avg. loss: 0.513765\n",
      "Total training time: 101.29 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292403, T: 255991294, Avg. loss: 0.513763\n",
      "Total training time: 101.93 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292408, T: 257524176, Avg. loss: 0.513764\n",
      "Total training time: 102.51 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292393, T: 259057058, Avg. loss: 0.513764\n",
      "Total training time: 103.10 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292400, T: 260589940, Avg. loss: 0.513764\n",
      "Total training time: 103.70 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292406, T: 262122822, Avg. loss: 0.513765\n",
      "Total training time: 104.35 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292393, T: 263655704, Avg. loss: 0.513765\n",
      "Total training time: 104.96 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292403, T: 265188586, Avg. loss: 0.513765\n",
      "Total training time: 105.53 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292416, T: 266721468, Avg. loss: 0.513764\n",
      "Total training time: 106.18 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292436, T: 268254350, Avg. loss: 0.513764\n",
      "Total training time: 106.81 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292432, T: 269787232, Avg. loss: 0.513764\n",
      "Total training time: 107.42 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292439, T: 271320114, Avg. loss: 0.513764\n",
      "Total training time: 108.02 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292437, T: 272852996, Avg. loss: 0.513764\n",
      "Total training time: 108.66 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292432, T: 274385878, Avg. loss: 0.513765\n",
      "Total training time: 109.26 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292415, T: 275918760, Avg. loss: 0.513765\n",
      "Total training time: 109.86 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292405, T: 277451642, Avg. loss: 0.513764\n",
      "Total training time: 110.46 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292405, T: 278984524, Avg. loss: 0.513765\n",
      "Total training time: 111.03 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292415, T: 280517406, Avg. loss: 0.513765\n",
      "Total training time: 111.66 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292436, T: 282050288, Avg. loss: 0.513764\n",
      "Total training time: 112.23 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292444, T: 283583170, Avg. loss: 0.513764\n",
      "Total training time: 112.80 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292433, T: 285116052, Avg. loss: 0.513764\n",
      "Total training time: 113.42 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292436, T: 286648934, Avg. loss: 0.513765\n",
      "Total training time: 114.02 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292418, T: 288181816, Avg. loss: 0.513765\n",
      "Total training time: 114.57 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292389, T: 289714698, Avg. loss: 0.513765\n",
      "Total training time: 115.12 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292389, T: 291247580, Avg. loss: 0.513765\n",
      "Total training time: 115.79 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292399, T: 292780462, Avg. loss: 0.513764\n",
      "Total training time: 116.39 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292393, T: 294313344, Avg. loss: 0.513764\n",
      "Total training time: 117.03 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292381, T: 295846226, Avg. loss: 0.513764\n",
      "Total training time: 117.65 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292393, T: 297379108, Avg. loss: 0.513764\n",
      "Total training time: 118.29 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292400, T: 298911990, Avg. loss: 0.513764\n",
      "Total training time: 118.93 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292387, T: 300444872, Avg. loss: 0.513765\n",
      "Total training time: 119.57 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292390, T: 301977754, Avg. loss: 0.513764\n",
      "Total training time: 120.18 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292378, T: 303510636, Avg. loss: 0.513765\n",
      "Total training time: 120.82 seconds.\n",
      "-- Epoch 199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 28.38, NNZs: 267036, Bias: 0.292397, T: 305043518, Avg. loss: 0.513765\n",
      "Total training time: 121.44 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292394, T: 306576400, Avg. loss: 0.513764\n",
      "Total training time: 122.02 seconds.\n",
      "Wall time: 2min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=200, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sgd = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7753248786597777"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy of Logistic Regression model is 77.53%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.76      0.77     15868\n",
      "          1       0.77      0.79      0.78     16067\n",
      "\n",
      "avg / total       0.78      0.78      0.78     31935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Voting Classifier using Ensemble Classification Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vc_1 = VotingClassifier(estimators=[('lr', lr), ('mnb', mnb), ('sgd', sgd)], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]-- Epoch 1\n",
      "Norm: 28.41, NNZs: 267036, Bias: 0.292537, T: 1532882, Avg. loss: 0.515294\n",
      "Total training time: 0.56 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.291639, T: 3065764, Avg. loss: 0.514029\n",
      "Total training time: 1.13 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.289855, T: 4598646, Avg. loss: 0.513956\n",
      "Total training time: 1.70 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.291020, T: 6131528, Avg. loss: 0.513894\n",
      "Total training time: 2.27 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.291986, T: 7664410, Avg. loss: 0.513846\n",
      "Total training time: 2.84 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.290954, T: 9197292, Avg. loss: 0.513854\n",
      "Total training time: 3.47 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.293357, T: 10730174, Avg. loss: 0.513831\n",
      "Total training time: 4.06 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 28.39, NNZs: 267036, Bias: 0.290142, T: 12263056, Avg. loss: 0.513845\n",
      "Total training time: 4.62 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.293043, T: 13795938, Avg. loss: 0.513811\n",
      "Total training time: 5.18 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.291624, T: 15328820, Avg. loss: 0.513810\n",
      "Total training time: 5.75 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292944, T: 16861702, Avg. loss: 0.513802\n",
      "Total training time: 6.29 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.293291, T: 18394584, Avg. loss: 0.513801\n",
      "Total training time: 6.85 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292309, T: 19927466, Avg. loss: 0.513793\n",
      "Total training time: 7.41 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292595, T: 21460348, Avg. loss: 0.513799\n",
      "Total training time: 7.97 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292901, T: 22993230, Avg. loss: 0.513797\n",
      "Total training time: 8.53 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292073, T: 24526112, Avg. loss: 0.513801\n",
      "Total training time: 9.09 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.291421, T: 26058994, Avg. loss: 0.513782\n",
      "Total training time: 9.63 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292664, T: 27591876, Avg. loss: 0.513798\n",
      "Total training time: 10.19 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.291793, T: 29124758, Avg. loss: 0.513791\n",
      "Total training time: 10.76 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.293111, T: 30657640, Avg. loss: 0.513781\n",
      "Total training time: 11.31 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292675, T: 32190522, Avg. loss: 0.513793\n",
      "Total training time: 11.86 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292482, T: 33723404, Avg. loss: 0.513782\n",
      "Total training time: 12.43 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292454, T: 35256286, Avg. loss: 0.513782\n",
      "Total training time: 13.01 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292260, T: 36789168, Avg. loss: 0.513786\n",
      "Total training time: 13.61 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292287, T: 38322050, Avg. loss: 0.513782\n",
      "Total training time: 14.21 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292163, T: 39854932, Avg. loss: 0.513778\n",
      "Total training time: 14.80 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292554, T: 41387814, Avg. loss: 0.513783\n",
      "Total training time: 15.39 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292692, T: 42920696, Avg. loss: 0.513776\n",
      "Total training time: 15.97 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292723, T: 44453578, Avg. loss: 0.513774\n",
      "Total training time: 16.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292560, T: 45986460, Avg. loss: 0.513780\n",
      "Total training time: 17.09 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292598, T: 47519342, Avg. loss: 0.513778\n",
      "Total training time: 17.65 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292453, T: 49052224, Avg. loss: 0.513775\n",
      "Total training time: 18.21 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292516, T: 50585106, Avg. loss: 0.513780\n",
      "Total training time: 18.78 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292725, T: 52117988, Avg. loss: 0.513776\n",
      "Total training time: 19.36 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292611, T: 53650870, Avg. loss: 0.513777\n",
      "Total training time: 19.98 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292425, T: 55183752, Avg. loss: 0.513774\n",
      "Total training time: 20.62 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292449, T: 56716634, Avg. loss: 0.513775\n",
      "Total training time: 21.26 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292429, T: 58249516, Avg. loss: 0.513774\n",
      "Total training time: 21.92 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292263, T: 59782398, Avg. loss: 0.513774\n",
      "Total training time: 22.50 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292121, T: 61315280, Avg. loss: 0.513773\n",
      "Total training time: 23.17 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292491, T: 62848162, Avg. loss: 0.513772\n",
      "Total training time: 23.79 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292426, T: 64381044, Avg. loss: 0.513774\n",
      "Total training time: 24.44 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292406, T: 65913926, Avg. loss: 0.513774\n",
      "Total training time: 25.04 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292327, T: 67446808, Avg. loss: 0.513775\n",
      "Total training time: 25.70 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292377, T: 68979690, Avg. loss: 0.513774\n",
      "Total training time: 26.33 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292604, T: 70512572, Avg. loss: 0.513771\n",
      "Total training time: 26.95 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292438, T: 72045454, Avg. loss: 0.513775\n",
      "Total training time: 27.50 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292435, T: 73578336, Avg. loss: 0.513772\n",
      "Total training time: 28.06 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292245, T: 75111218, Avg. loss: 0.513772\n",
      "Total training time: 28.62 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292174, T: 76644100, Avg. loss: 0.513772\n",
      "Total training time: 29.17 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292356, T: 78176982, Avg. loss: 0.513771\n",
      "Total training time: 29.72 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292411, T: 79709864, Avg. loss: 0.513770\n",
      "Total training time: 30.28 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292492, T: 81242746, Avg. loss: 0.513771\n",
      "Total training time: 30.84 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292603, T: 82775628, Avg. loss: 0.513770\n",
      "Total training time: 31.39 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292569, T: 84308510, Avg. loss: 0.513769\n",
      "Total training time: 31.94 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292380, T: 85841392, Avg. loss: 0.513773\n",
      "Total training time: 32.51 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292337, T: 87374274, Avg. loss: 0.513771\n",
      "Total training time: 33.08 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292327, T: 88907156, Avg. loss: 0.513771\n",
      "Total training time: 33.62 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292321, T: 90440038, Avg. loss: 0.513769\n",
      "Total training time: 34.18 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292315, T: 91972920, Avg. loss: 0.513771\n",
      "Total training time: 34.73 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292319, T: 93505802, Avg. loss: 0.513768\n",
      "Total training time: 35.28 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292203, T: 95038684, Avg. loss: 0.513770\n",
      "Total training time: 35.84 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292350, T: 96571566, Avg. loss: 0.513770\n",
      "Total training time: 36.40 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292484, T: 98104448, Avg. loss: 0.513770\n",
      "Total training time: 36.95 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292511, T: 99637330, Avg. loss: 0.513769\n",
      "Total training time: 37.50 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292541, T: 101170212, Avg. loss: 0.513768\n",
      "Total training time: 38.06 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292535, T: 102703094, Avg. loss: 0.513770\n",
      "Total training time: 38.64 seconds.\n",
      "-- Epoch 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 28.38, NNZs: 267036, Bias: 0.292545, T: 104235976, Avg. loss: 0.513768\n",
      "Total training time: 39.21 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292574, T: 105768858, Avg. loss: 0.513770\n",
      "Total training time: 39.79 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292516, T: 107301740, Avg. loss: 0.513769\n",
      "Total training time: 40.38 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292493, T: 108834622, Avg. loss: 0.513771\n",
      "Total training time: 40.95 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292577, T: 110367504, Avg. loss: 0.513768\n",
      "Total training time: 41.64 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292493, T: 111900386, Avg. loss: 0.513769\n",
      "Total training time: 42.30 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292472, T: 113433268, Avg. loss: 0.513769\n",
      "Total training time: 42.87 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292470, T: 114966150, Avg. loss: 0.513769\n",
      "Total training time: 43.44 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292537, T: 116499032, Avg. loss: 0.513769\n",
      "Total training time: 43.99 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292320, T: 118031914, Avg. loss: 0.513769\n",
      "Total training time: 44.54 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292524, T: 119564796, Avg. loss: 0.513766\n",
      "Total training time: 45.11 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292544, T: 121097678, Avg. loss: 0.513769\n",
      "Total training time: 45.66 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292590, T: 122630560, Avg. loss: 0.513767\n",
      "Total training time: 46.22 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292448, T: 124163442, Avg. loss: 0.513768\n",
      "Total training time: 46.78 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292387, T: 125696324, Avg. loss: 0.513769\n",
      "Total training time: 47.33 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292351, T: 127229206, Avg. loss: 0.513768\n",
      "Total training time: 47.90 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292398, T: 128762088, Avg. loss: 0.513767\n",
      "Total training time: 48.45 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292449, T: 130294970, Avg. loss: 0.513769\n",
      "Total training time: 49.02 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292419, T: 131827852, Avg. loss: 0.513768\n",
      "Total training time: 49.60 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292352, T: 133360734, Avg. loss: 0.513769\n",
      "Total training time: 50.17 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292381, T: 134893616, Avg. loss: 0.513766\n",
      "Total training time: 50.72 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292466, T: 136426498, Avg. loss: 0.513767\n",
      "Total training time: 51.30 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292446, T: 137959380, Avg. loss: 0.513768\n",
      "Total training time: 51.88 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292435, T: 139492262, Avg. loss: 0.513769\n",
      "Total training time: 52.44 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292430, T: 141025144, Avg. loss: 0.513766\n",
      "Total training time: 53.04 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292459, T: 142558026, Avg. loss: 0.513767\n",
      "Total training time: 53.61 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292492, T: 144090908, Avg. loss: 0.513767\n",
      "Total training time: 54.18 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292422, T: 145623790, Avg. loss: 0.513768\n",
      "Total training time: 54.91 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292429, T: 147156672, Avg. loss: 0.513767\n",
      "Total training time: 55.47 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292332, T: 148689554, Avg. loss: 0.513767\n",
      "Total training time: 56.05 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292331, T: 150222436, Avg. loss: 0.513767\n",
      "Total training time: 56.60 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292355, T: 151755318, Avg. loss: 0.513766\n",
      "Total training time: 57.15 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292371, T: 153288200, Avg. loss: 0.513766\n",
      "Total training time: 57.70 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292369, T: 154821082, Avg. loss: 0.513767\n",
      "Total training time: 58.27 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292396, T: 156353964, Avg. loss: 0.513765\n",
      "Total training time: 58.82 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292420, T: 157886846, Avg. loss: 0.513766\n",
      "Total training time: 59.38 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292449, T: 159419728, Avg. loss: 0.513766\n",
      "Total training time: 59.94 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292420, T: 160952610, Avg. loss: 0.513767\n",
      "Total training time: 60.50 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292443, T: 162485492, Avg. loss: 0.513767\n",
      "Total training time: 61.06 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292374, T: 164018374, Avg. loss: 0.513767\n",
      "Total training time: 61.61 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292326, T: 165551256, Avg. loss: 0.513766\n",
      "Total training time: 62.16 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292315, T: 167084138, Avg. loss: 0.513767\n",
      "Total training time: 62.72 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292385, T: 168617020, Avg. loss: 0.513766\n",
      "Total training time: 63.28 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292396, T: 170149902, Avg. loss: 0.513765\n",
      "Total training time: 63.84 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292418, T: 171682784, Avg. loss: 0.513766\n",
      "Total training time: 64.39 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292419, T: 173215666, Avg. loss: 0.513765\n",
      "Total training time: 64.95 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292420, T: 174748548, Avg. loss: 0.513767\n",
      "Total training time: 65.49 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292430, T: 176281430, Avg. loss: 0.513766\n",
      "Total training time: 66.09 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292338, T: 177814312, Avg. loss: 0.513767\n",
      "Total training time: 66.71 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292374, T: 179347194, Avg. loss: 0.513765\n",
      "Total training time: 67.33 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292375, T: 180880076, Avg. loss: 0.513767\n",
      "Total training time: 68.02 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292333, T: 182412958, Avg. loss: 0.513765\n",
      "Total training time: 68.68 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292334, T: 183945840, Avg. loss: 0.513767\n",
      "Total training time: 69.25 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292390, T: 185478722, Avg. loss: 0.513765\n",
      "Total training time: 69.82 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292367, T: 187011604, Avg. loss: 0.513766\n",
      "Total training time: 70.37 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292400, T: 188544486, Avg. loss: 0.513766\n",
      "Total training time: 70.94 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292386, T: 190077368, Avg. loss: 0.513766\n",
      "Total training time: 71.49 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292377, T: 191610250, Avg. loss: 0.513766\n",
      "Total training time: 72.04 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292429, T: 193143132, Avg. loss: 0.513766\n",
      "Total training time: 72.69 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292394, T: 194676014, Avg. loss: 0.513766\n",
      "Total training time: 73.26 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292404, T: 196208896, Avg. loss: 0.513765\n",
      "Total training time: 73.81 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292456, T: 197741778, Avg. loss: 0.513765\n",
      "Total training time: 74.38 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292454, T: 199274660, Avg. loss: 0.513766\n",
      "Total training time: 74.93 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292399, T: 200807542, Avg. loss: 0.513767\n",
      "Total training time: 75.48 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292367, T: 202340424, Avg. loss: 0.513766\n",
      "Total training time: 76.08 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292369, T: 203873306, Avg. loss: 0.513766\n",
      "Total training time: 76.64 seconds.\n",
      "-- Epoch 134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 28.38, NNZs: 267036, Bias: 0.292385, T: 205406188, Avg. loss: 0.513766\n",
      "Total training time: 77.32 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292400, T: 206939070, Avg. loss: 0.513766\n",
      "Total training time: 77.90 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292408, T: 208471952, Avg. loss: 0.513765\n",
      "Total training time: 78.46 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292388, T: 210004834, Avg. loss: 0.513766\n",
      "Total training time: 79.03 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292357, T: 211537716, Avg. loss: 0.513766\n",
      "Total training time: 79.58 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292363, T: 213070598, Avg. loss: 0.513765\n",
      "Total training time: 80.13 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292353, T: 214603480, Avg. loss: 0.513765\n",
      "Total training time: 80.69 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292378, T: 216136362, Avg. loss: 0.513765\n",
      "Total training time: 81.25 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292373, T: 217669244, Avg. loss: 0.513765\n",
      "Total training time: 81.80 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292363, T: 219202126, Avg. loss: 0.513766\n",
      "Total training time: 82.35 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292370, T: 220735008, Avg. loss: 0.513765\n",
      "Total training time: 82.90 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292380, T: 222267890, Avg. loss: 0.513764\n",
      "Total training time: 83.49 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292365, T: 223800772, Avg. loss: 0.513765\n",
      "Total training time: 84.06 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292395, T: 225333654, Avg. loss: 0.513764\n",
      "Total training time: 84.73 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292348, T: 226866536, Avg. loss: 0.513765\n",
      "Total training time: 85.37 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292346, T: 228399418, Avg. loss: 0.513765\n",
      "Total training time: 85.94 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292377, T: 229932300, Avg. loss: 0.513765\n",
      "Total training time: 86.51 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292406, T: 231465182, Avg. loss: 0.513765\n",
      "Total training time: 87.09 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292388, T: 232998064, Avg. loss: 0.513765\n",
      "Total training time: 87.66 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292377, T: 234530946, Avg. loss: 0.513765\n",
      "Total training time: 88.25 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292395, T: 236063828, Avg. loss: 0.513766\n",
      "Total training time: 88.81 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292403, T: 237596710, Avg. loss: 0.513765\n",
      "Total training time: 89.37 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292406, T: 239129592, Avg. loss: 0.513765\n",
      "Total training time: 89.93 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292400, T: 240662474, Avg. loss: 0.513764\n",
      "Total training time: 90.49 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292395, T: 242195356, Avg. loss: 0.513765\n",
      "Total training time: 91.03 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292387, T: 243728238, Avg. loss: 0.513765\n",
      "Total training time: 91.59 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292416, T: 245261120, Avg. loss: 0.513764\n",
      "Total training time: 92.16 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292451, T: 246794002, Avg. loss: 0.513765\n",
      "Total training time: 92.74 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292425, T: 248326884, Avg. loss: 0.513765\n",
      "Total training time: 93.31 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292397, T: 249859766, Avg. loss: 0.513766\n",
      "Total training time: 93.92 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292379, T: 251392648, Avg. loss: 0.513765\n",
      "Total training time: 94.59 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292381, T: 252925530, Avg. loss: 0.513765\n",
      "Total training time: 95.18 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292399, T: 254458412, Avg. loss: 0.513765\n",
      "Total training time: 95.74 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292400, T: 255991294, Avg. loss: 0.513765\n",
      "Total training time: 96.30 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292400, T: 257524176, Avg. loss: 0.513765\n",
      "Total training time: 96.87 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292426, T: 259057058, Avg. loss: 0.513765\n",
      "Total training time: 97.42 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292437, T: 260589940, Avg. loss: 0.513764\n",
      "Total training time: 97.99 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292441, T: 262122822, Avg. loss: 0.513764\n",
      "Total training time: 98.54 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292430, T: 263655704, Avg. loss: 0.513764\n",
      "Total training time: 99.09 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292410, T: 265188586, Avg. loss: 0.513765\n",
      "Total training time: 99.65 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292398, T: 266721468, Avg. loss: 0.513765\n",
      "Total training time: 100.21 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292383, T: 268254350, Avg. loss: 0.513764\n",
      "Total training time: 100.78 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292389, T: 269787232, Avg. loss: 0.513764\n",
      "Total training time: 101.36 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292404, T: 271320114, Avg. loss: 0.513765\n",
      "Total training time: 101.93 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292391, T: 272852996, Avg. loss: 0.513764\n",
      "Total training time: 102.65 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292373, T: 274385878, Avg. loss: 0.513764\n",
      "Total training time: 103.23 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292384, T: 275918760, Avg. loss: 0.513764\n",
      "Total training time: 103.80 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292377, T: 277451642, Avg. loss: 0.513764\n",
      "Total training time: 104.37 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292348, T: 278984524, Avg. loss: 0.513765\n",
      "Total training time: 104.94 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292340, T: 280517406, Avg. loss: 0.513764\n",
      "Total training time: 105.52 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292365, T: 282050288, Avg. loss: 0.513764\n",
      "Total training time: 106.17 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292387, T: 283583170, Avg. loss: 0.513764\n",
      "Total training time: 106.72 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292411, T: 285116052, Avg. loss: 0.513764\n",
      "Total training time: 107.30 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292418, T: 286648934, Avg. loss: 0.513765\n",
      "Total training time: 107.86 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292427, T: 288181816, Avg. loss: 0.513765\n",
      "Total training time: 108.43 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292417, T: 289714698, Avg. loss: 0.513765\n",
      "Total training time: 109.00 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292409, T: 291247580, Avg. loss: 0.513764\n",
      "Total training time: 109.59 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292408, T: 292780462, Avg. loss: 0.513764\n",
      "Total training time: 110.23 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292425, T: 294313344, Avg. loss: 0.513764\n",
      "Total training time: 110.82 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292418, T: 295846226, Avg. loss: 0.513765\n",
      "Total training time: 111.42 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292424, T: 297379108, Avg. loss: 0.513764\n",
      "Total training time: 112.01 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292415, T: 298911990, Avg. loss: 0.513764\n",
      "Total training time: 112.57 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292420, T: 300444872, Avg. loss: 0.513764\n",
      "Total training time: 113.16 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292401, T: 301977754, Avg. loss: 0.513764\n",
      "Total training time: 113.76 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292416, T: 303510636, Avg. loss: 0.513764\n",
      "Total training time: 114.39 seconds.\n",
      "-- Epoch 199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 28.38, NNZs: 267036, Bias: 0.292418, T: 305043518, Avg. loss: 0.513764\n",
      "Total training time: 114.99 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 28.38, NNZs: 267036, Bias: 0.292412, T: 306576400, Avg. loss: 0.513764\n",
      "Total training time: 115.61 seconds.\n",
      "Wall time: 3min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=42, solver='liblinear', tol=0.0001,\n",
       "          verbose=1, warm_start=False)), ('mnb', Multinom...='l2', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=2, warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=1, voting='soft', weights=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_vc_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred_em_1 = model_vc_1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y_array_em_1 = y_test.as_matrix(columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25534"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same_em_1 = 0\n",
    "for i in range(y_test.count()):\n",
    "    if(y_array_em_1[i] == y_pred_em_1[i]):\n",
    "        same_em_1 = same_em_1+1\n",
    "same_em_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80     15868\n",
      "          1       0.80      0.80      0.80     16067\n",
      "\n",
      "avg / total       0.80      0.80      0.80     31935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_em_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7995616095193362"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vc_1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The accuracy of Voting Classificatin Model is 79.95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, manually testing the model, of the value of array[0] is 1, then the sentence given in Positive, and if the value of array[0] is 0, then the sentence given is Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testing = vectorizer.transform([\"I am very happy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred_testing = model_vc_1.predict(X_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_testing = vectorizer.transform([\"I am very sad\"])\n",
    "y_pred_testing = model_vc_1.predict(X_testing)\n",
    "y_pred_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Converting the trained Vectorizer and Voting Classifier model into pickle files using joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlmodel.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer, \"tfidf.pkl\", protocol = 2)\n",
    "joblib.dump(model_vc_1, \"mlmodel.pkl\", protocol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Now these models can be imported anywhere for getting output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
